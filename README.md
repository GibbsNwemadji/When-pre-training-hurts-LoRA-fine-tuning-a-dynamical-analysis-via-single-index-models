# When-pre-training-hurts-LoRA-fine-tuning-a-dynamical-analysis-via-single-index-models
We provide a theoretical and experimental study of low-rank adaptation (LoRA) fine-tuning, showing that strong pre-training can paradoxically slow down optimization, even when the source and target tasks are well aligned.
